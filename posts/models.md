<header><title>Operational mental models</title></header>

# Operational mental models

Mental models are how we understand the world, typically via shortcuts that our brains use to simplify our complex reality. These shortcuts are not necessarily helpful. What evolved to work for a primate on the African savannah may not apply to a modern society. A bias towards seeing patterns in noise, which helped your ancestor spot a camouflaged predator, might today lead you to see conspiracies in random events.

There are plenty of existing guides to mental models. Some that I've found useful include [Farnam Street](https://fs.blog/mental-models/) and [Conceptually](https://conceptually.org/), and the books [Algorithms To Live By](https://www.goodreads.com/book/show/25666050-algorithms-to-live-by) and [Thinking, Fast & Slow](https://www.goodreads.com/book/show/11468377-thinking-fast-and-slow).

But I've also found that sometimes these err towards presenting models in an abstract way that is hard to apply in practice. If you're Warren Buffett (a Farnam Street hero), you have both the time and the motivation to really refine your thinking on each very specific investment decision, stamping out all cognitive bias in pursuit of a perfect model of the world. 

Most people probably aren't facing big, abstract, strategic questions like that very often. Patrick Collison captured this really well during an [interview with Tim Ferriss](https://tim.blog/2018/12/24/the-tim-ferriss-show-patrick-collison/):

> **TF**: But could you talk about your framework for making decisions or how you have cultivated making faster decisions? Because this is I think so important. [...]

> **PC**: Yes. Let me think for a second. I actually think that decision making is probably a little bit overrated as a question or an area of study in that – investors, obviously, this is their job, right? And that’s all they can do in some sense, invest or not. ... But in some simplified model, it’s like, “Do you click the button or not? Do you pull the lever or not?” **I actually think that in our lives, things rarely have that character**. Sometimes you have a true binary decision like, “Do I go to this college or that college?” “Do I take this job offer or not?” And it’s an investing or investment-like decision.
> But it’s not usually that. And I think that the question I would encourage people to think more about is, “How do I get to make better decisions?” as in, “How do I make sure the decisions I’m confronted with end up being better?” **It’s not like, “How should I choose between option A and B,” but, “How do I make sure that both options A and B are as good as possible, and there’s also a C, D, and E, and that those options are great too?”** ...
> It’s less on how do you make a decision and more about **how do you jolt yourself out of the particular furrows that you’re in and realize the possibility space and the world is just so much bigger than perhaps people are thinking about**. It happens to all of us. It happens to me, that your horizons narrow. You get stuck in your existing models. You get used to conceiving of the world and the options in front of you in a particular way. And it’s more about how do you repeatedly pull yourself out of that.

So with that framing in mind, I wanted to pull together a few sets of models I've found to be actually helpful in less abstract, more operational settings.

### Bike-shedding, availability bias
[Bike-shedding](https://en.wikipedia.org/wiki/Law_of_triviality) is the tendency to fixate on and over-analyse trivial but understandable aspects of a problem, while ignoring the more important but less familiar parts. Related, [availability bias](https://en.wikipedia.org/wiki/Availability_heuristic) is the tendency to over-estimate the importance of things that you can easily visualise and call to mind, whether because they're visceral (like a shark attack) or simply because you're already familiar with them.

In practice, I've seen this intersect with the fact that every organisation and team has some specialised areas of strength. It's much easier to get into the weeds of the areas you're comfortable with, than think about the potentially much more important areas that don't align with your organisational culture or interests. For example, a software company is typically full of engineers and product managers who care a lot about technical implementation and usability, and are good at talking and thinking about those things. They might assume as a result that those factors are the most likely to determine business success. But actually, perhaps marketing and distribution are equally vital to success in a consumer product, and executive political relationships are vital in an enterprise setting. There's a point at which you need to stop further optimising the technical product approach, and focus on the other dimensions surrounding that.

### Boiling frog
The [boiling frog](https://en.wikipedia.org/wiki/Boiling_frog) is the tale of a frog placed in a pot that is slowly heated, eventually boiling because the temperature increase is never sharp enough to shock it into jumping out. (Luckily for frogs, it doesn't seem to actually be true!) The metaphorical lesson is that gradually deteriorating situations can lead to dire outcomes if you don't notice the ongoing change -- and you probably won't if you're in the middle of it.

In practice, I've fallen victim to this when pursuing difficult projects involving new technology. In one case, we were gradually discovering new issues that impacted the quality of the solution. In another case, we were fixing issues, but not fast enough to be delivering something truly excellent and meet the bar we held for ourselves. Both times, it took an outside perspective to call out that the frog was boiling and we needed a more radical re-think. In later projects, we tried to avoid the frog-boil by imposing clearer interim milestones (to create accountability if things were gradually slipping), and by inviting more external scrutiny by people who weren't involved day-to-day.

### Cargo cults


### Fundamental attribution bias, Hanlon's razor


### Power laws, Pareto principle