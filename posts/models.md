<header><title>Operational mental models</title></header>

# Operational mental models

Mental models are how we understand the world, typically via shortcuts that our brains use to simplify our complex reality. These shortcuts are not necessarily helpful. What evolved to work for a primate on the African savannah may not apply to a modern society. A bias towards seeing patterns in noise, which helped your ancestor spot a camouflaged predator, might today lead you to see conspiracies in random events.

There are plenty of existing guides to mental models. Some that I've found useful include [Farnam Street](https://fs.blog/mental-models/) and [Conceptually](https://conceptually.org/), and the books [Algorithms To Live By](https://www.goodreads.com/book/show/25666050-algorithms-to-live-by) and [Thinking, Fast & Slow](https://www.goodreads.com/book/show/11468377-thinking-fast-and-slow).

But I've also found that sometimes these err towards presenting models in an abstract way that is hard to apply in practice. If you're Warren Buffett (a Farnam Street hero), you have both the time and the motivation to really refine your thinking on each very specific investment decision, stamping out all cognitive bias in pursuit of a perfect model of the world. 

Most people probably aren't facing big, abstract, strategic questions like that very often. Patrick Collison captured this really well during an [interview with Tim Ferriss](https://tim.blog/2018/12/24/the-tim-ferriss-show-patrick-collison/):

> **TF**: But could you talk about your framework for making decisions or how you have cultivated making faster decisions? Because this is I think so important. [...]

> **PC**: Yes. Let me think for a second. I actually think that decision making is probably a little bit overrated as a question or an area of study in that – investors, obviously, this is their job, right? And that’s all they can do in some sense, invest or not. ... But in some simplified model, it’s like, “Do you click the button or not? Do you pull the lever or not?” **I actually think that in our lives, things rarely have that character**. Sometimes you have a true binary decision like, “Do I go to this college or that college?” “Do I take this job offer or not?” And it’s an investing or investment-like decision.
> But it’s not usually that. And I think that the question I would encourage people to think more about is, “How do I get to make better decisions?” as in, “How do I make sure the decisions I’m confronted with end up being better?” **It’s not like, “How should I choose between option A and B,” but, “How do I make sure that both options A and B are as good as possible, and there’s also a C, D, and E, and that those options are great too?”** ...
> It’s less on how do you make a decision and more about **how do you jolt yourself out of the particular furrows that you’re in and realize the possibility space and the world is just so much bigger than perhaps people are thinking about**. It happens to all of us. It happens to me, that your horizons narrow. You get stuck in your existing models. You get used to conceiving of the world and the options in front of you in a particular way. And it’s more about how do you repeatedly pull yourself out of that.

So with that framing in mind, I wanted to pull together a few sets of models I've found to be actually helpful in less abstract, more operational settings.

### Bike-shedding and availability bias
[Bike-shedding](https://en.wikipedia.org/wiki/Law_of_triviality) is the tendency to fixate on and over-analyse trivial but understandable aspects of a problem, while ignoring the more important but less familiar parts. Related, [availability bias](https://en.wikipedia.org/wiki/Availability_heuristic) is the tendency to over-estimate the importance of things that you can easily visualise and call to mind, whether because they're visceral (like a shark attack) or simply because you're already familiar with them.

In practice, I've seen this intersect with the fact that every organisation and team has some specialised areas of strength. It's much easier to get into the weeds of the areas you're comfortable with, than think about the potentially much more important areas that don't align with your organisational culture or interests. For example, a software company is typically full of engineers and product managers who care a lot about technical implementation and usability, and are good at talking and thinking about those things. They might assume as a result that those factors are the most likely to determine business success. But actually, perhaps marketing and distribution are equally vital to success in a consumer product, and executive political relationships are vital in an enterprise setting. There's a point at which you need to stop further optimising the technical product approach, and focus on the other dimensions surrounding that.

### Boiling frog
The [boiling frog](https://en.wikipedia.org/wiki/Boiling_frog) is the tale of a frog placed in a pot that is slowly heated, eventually boiling because the temperature increase is never sharp enough to shock it into jumping out. (Luckily for frogs, it doesn't seem to actually be true!) The metaphorical lesson is that gradually deteriorating situations can lead to dire outcomes if you don't notice the ongoing change -- and you probably won't if you're in the middle of it.

In practice, I've fallen victim to this when pursuing difficult projects involving new technology. In one case, we were gradually discovering new issues that impacted the quality of the solution. In another case, we were fixing issues, but not fast enough to be delivering something truly excellent and meet the bar we held for ourselves. Both times, it took an outside perspective to call out that the frog was boiling and we needed a more radical re-think. In later projects, we tried to avoid the frog-boil by imposing clearer interim milestones (to create accountability if things were gradually slipping), and by inviting more external scrutiny by people who weren't involved day-to-day.

### Cargo cults
[Cargo cults](https://en.wikipedia.org/wiki/Cargo_cult) are a super-interesting cultural practice in some Pacific islands that saw huge influxes of cargo and material goods from the American and Japanese militaries operating during WWII. After the war, some of these societies developed practices that they thought would guarantee the flow of cargo would continue: clearing airstrips, building mock aircraft control towers, even carving flight-controller headphones from wood. Of course, no new planes or cargo ever arrived.

Modern organisations often perform something pretty close to this, when they copy the superficial practices, but not the essential nature, of a more successful organisation they aspire to match. You've probably seen examples like:
- An organisation that adopts all the terminology and meeting process of Agile software development, but retains a complex oversight process and doesn't actually empower teams to rapidly deliver functionality for users.
- An organisation that tries to promote innovation by copying the beanbag-rich physical workspaces of somewhere like Google, but doesn't have any mechanisms for employees to pilot new ideas.

It's always worth thinking about what is actually underpinning successful execution in a domain. It's much more likely to be a deep organisational culture than any of the particular practices or artefacts that an organisation uses at a given point.

### Fundamental attribution bias and Hanlon's razor
[Fundamental attribution bias](https://en.wikipedia.org/wiki/Fundamental_attribution_error) is our tendency to attribute other people's behaviour to their intrinsic personality, while often explaining our own behaviour in terms of external circumstances and constraints. Wikipedia's example is a good one: if you cut me off in traffic, it's because you're just a jerk; but if I cut you off, it's because I'm running late and it can't be helped. I let myself off the hook. [Hanlon's razor](https://en.wikipedia.org/wiki/Hanlon%27s_razor) is another way of thinking about this: "Never attribute to malice that which can be adequately explained by stupidity." (Or less sharply but more generally useful: “Never attribute to malice or stupidity that which can be explained by moderately rational individuals following incentives in a complex system of interactions.”)

The lesson I've found here is that when dealing with counterparts inside large, complex organisations, it's often tempting to write off any misalignment between you and them as a critical character fault on their part. They just don't get it! Why can't they see what we're trying to do here?! They're so lazy! And so on. But really, this is almost never fair. And even if it is fair, it isn't really helpful: nobody is going to think of themselves in those terms, so it's much more useful to think about what their incentives and motivations are. What do they care about, why is that misaligned with you, and is there a way to find common ground that still advances your objective? 

### Pareto principle and power laws
The [Pareto principle](https://en.wikipedia.org/wiki/Pareto_principle), aka the 80-20 rule, is well-known and I won't dwell on explaining it. [Power laws](https://en.wikipedia.org/wiki/Power_law) are in some sense a more general expression of the same idea: in a lot of phenonema, a small number of input items will dominate most of the output. In _Zero to One_, Peter Thiel has a good description of power-law returns in venture capital, drawing on his VC firm Founders Fund: Facebook (its #1 investment) is worth more than all its other investments combined, and Palantir (its #2 investment) is worth more than investments #3-N combined.

While most people intellectually know about this kind of relationship, I think it's quite hard to execute on. This is partly due to bike-shedding: it's often easier to work on the moderate-priority thing you know how to tackle, rather than something else that is high-priority, but perhaps poorly defined or unfamiliar. Figuring out when you're avoiding something for this reason -- perhaps by stepping back to first principles about what is really going to matter on a given project -- can be helpful. 

I've also found that it occurs in product settings. It's hard to balance the contradictory ideas of power laws (do only the things that are wildly important) and "sweating the details" (do the many small things that are needed to ensure the user has a delightful experience). A way to resolve the tension is to try to avoid letting the details becoming an excuse for not shipping and proving out whether the core of an idea really matters. Steve Jobs was famously detail-oriented, but the iPhone still shipped without third-party apps, copy and paste, video recording, or GPS. Shipping the core of what you believe is a wildly important idea, and then filling in the details if it gets traction, is likely preferable to deferring so you can keep spending time on details in advance. Figuring out what's genuinely a blocking issue for a user, or conversely what will genuinely get a new user excited enough to use the system in spite of its defects, is really important.